{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c59e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.cuda import amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0442727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础配置\n",
    "seed = 42\n",
    "batch_size = 128\n",
    "num_workers = 4 if torch.cuda.is_available() else 2\n",
    "epochs = 3  # 若想取得更好精度，可提高到10+\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 5e-4\n",
    "use_amp = True\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 数据路径：优先使用已有CIFAR-10缓存\n",
    "data_root_candidates = [Path('../cnn_cifar/data'), Path('./data'), Path('../data')]\n",
    "data_root = None\n",
    "for cand in data_root_candidates:\n",
    "    if (cand / 'cifar-10-batches-py').exists():\n",
    "        data_root = cand\n",
    "        break\n",
    "if data_root is None:\n",
    "    data_root = data_root_candidates[0]\n",
    "    download_flag = True\n",
    "else:\n",
    "    download_flag = False\n",
    "\n",
    "print(f\"Data root: {data_root.resolve()} | download: {download_flag}\")\n",
    "\n",
    "weights_dir = Path('./datasets-readonly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7118201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集与数据增强\n",
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std = (0.2470, 0.2435, 0.2616)\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "test_tf = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "train_set = datasets.CIFAR10(root=data_root, train=True, download=download_flag, transform=train_tf)\n",
    "test_set = datasets.CIFAR10(root=data_root, train=False, download=download_flag, transform=test_tf)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, persistent_workers=True if num_workers > 0 else False)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, persistent_workers=True if num_workers > 0 else False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82abbc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型构建与训练工具\n",
    "PRETRAINED_FILES = {\n",
    "    'alexnet': weights_dir / 'alexnet_pretrained_pytorch.pth',\n",
    "    'vgg16': weights_dir / 'vgg16_pretrained_pytorch.pth',\n",
    "    'resnet18': weights_dir / 'resnet18_pretrained_pytorch.pth',\n",
    "}\n",
    "\n",
    "def build_model(name: str, num_classes: int = 10) -> nn.Module:\n",
    "    name = name.lower()\n",
    "    if name not in PRETRAINED_FILES:\n",
    "        raise ValueError(f'Unsupported model: {name}')\n",
    "    weights_path = PRETRAINED_FILES[name]\n",
    "\n",
    "    if name == 'alexnet':\n",
    "        model = models.alexnet(weights=None)\n",
    "    elif name == 'vgg16':\n",
    "        model = models.vgg16(weights=None)\n",
    "    elif name == 'resnet18':\n",
    "        model = models.resnet18(weights=None)\n",
    "\n",
    "    if weights_path.exists():\n",
    "        state = torch.load(weights_path, map_location='cpu')\n",
    "        missing = model.load_state_dict(state, strict=False)\n",
    "        if missing.missing_keys:\n",
    "            print(f'[warn] Missing keys while loading {name}: {missing.missing_keys}')\n",
    "        if missing.unexpected_keys:\n",
    "            print(f'[warn] Unexpected keys while loading {name}: {missing.unexpected_keys}')\n",
    "    else:\n",
    "        print(f'[info] Local weights not found for {name}, falling back to torchvision pretrained.')\n",
    "        if name == 'alexnet':\n",
    "            model = models.alexnet(weights=models.AlexNet_Weights.IMAGENET1K_V1)\n",
    "        elif name == 'vgg16':\n",
    "            model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "        elif name == 'resnet18':\n",
    "            model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "    # 替换分类头\n",
    "    if name in ('alexnet', 'vgg16'):\n",
    "        in_features = model.classifier[6].in_features\n",
    "        model.classifier[6] = nn.Linear(in_features, num_classes)\n",
    "    else:  # resnet18\n",
    "        in_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def set_trainable_layers(model: nn.Module, finetune_backbone: bool) -> None:\n",
    "    # 仅分类头可训练，或全部可训练\n",
    "    for name, param in model.named_parameters():\n",
    "        is_head = name.startswith('classifier') or name.startswith('fc')\n",
    "        param.requires_grad = finetune_backbone or is_head\n",
    "\n",
    "def accuracy_from_logits(logits: torch.Tensor, labels: torch.Tensor) -> float:\n",
    "    preds = logits.argmax(dim=1)\n",
    "    correct = (preds == labels).sum().item()\n",
    "    return correct / labels.size(0)\n",
    "\n",
    "def train_one_epoch(model: nn.Module, loader: DataLoader, criterion, optimizer, scaler: amp.GradScaler) -> Dict[str, float]:\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        with amp.autocast(enabled=use_amp and device.type == 'cuda'):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        running_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return {\n",
    "        'loss': running_loss / total,\n",
    "        'acc': running_correct / total,\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader, criterion) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        with amp.autocast(enabled=use_amp and device.type == 'cuda'):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        running_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return {\n",
    "        'loss': running_loss / total,\n",
    "        'acc': running_correct / total,\n",
    "    }\n",
    "\n",
    "def run_experiment(model_name: str, finetune_backbone: bool) -> Dict[str, float]:\n",
    "    model = build_model(model_name).to(device)\n",
    "    set_trainable_layers(model, finetune_backbone)\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = optim.AdamW(params, lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scaler = amp.GradScaler(enabled=use_amp and device.type == 'cuda')\n",
    "\n",
    "    history = []\n",
    "    best_acc = 0.0\n",
    "    start_time = time.time()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_metrics = train_one_epoch(model, train_loader, criterion, optimizer, scaler)\n",
    "        val_metrics = evaluate(model, test_loader, criterion)\n",
    "        scheduler.step()\n",
    "        best_acc = max(best_acc, val_metrics['acc'])\n",
    "        history.append({**train_metrics, **{'val_loss': val_metrics['loss'], 'val_acc': val_metrics['acc']}})\n",
    "        print(f\"[ {model_name} | finetune_backbone={finetune_backbone} ] Epoch {epoch}/{epochs} | \"\n",
    "              f\"train_loss={train_metrics['loss']:.4f}, train_acc={train_metrics['acc']:.4f}, \"\n",
    "              f\"val_loss={val_metrics['loss']:.4f}, val_acc={val_metrics['acc']:.4f}\")\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'finetune_backbone': finetune_backbone,\n",
    "        'best_acc': best_acc,\n",
    "        'last_acc': history[-1]['val_acc'],\n",
    "        'elapsed_sec': elapsed,\n",
    "        'history': history,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe76ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行实验：三种模型 × 两种策略\n",
    "model_list = ['alexnet', 'vgg16', 'resnet18']\n",
    "results: List[Dict] = []\n",
    "\n",
    "for model_name in model_list:\n",
    "    for finetune_backbone in (True, False):\n",
    "        print(f\"===== Running {model_name} | finetune_backbone={finetune_backbone} =====\")\n",
    "        res = run_experiment(model_name, finetune_backbone)\n",
    "        results.append({k: v for k, v in res.items() if k != 'history'})\n",
    "\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_df['best_acc'] = (summary_df['best_acc'] * 100).round(2)\n",
    "summary_df['last_acc'] = (summary_df['last_acc'] * 100).round(2)\n",
    "summary_df['elapsed_sec'] = summary_df['elapsed_sec'].round(1)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32280fe1",
   "metadata": {},
   "source": [
    "## 提示与可选改进\n",
    "- 如需更高精度，将`epochs`提高到10-30，并适当调大学习率预热或使用更强数据增强（MixUp/CutMix/随机擦除等）。\n",
    "- 如果希望仅替换最后一层而冻结中间全连接层，可在`set_trainable_layers`中改为只开放`classifier.6`或`fc`层。\n",
    "- 若本地`./datasets-readonly`缺失预训练权重，代码会自动回退到torchvision预训练（需要网络可用）。\n",
    "- 运行结束后可根据`summary_df`中的精度比较两种策略表现。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
