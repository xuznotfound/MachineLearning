#!/usr/bin/env python3
"""
æ”¹è¿›ç‰ˆLSTMæ¨¡å‹ - å¾·é‡Œæ°”å€™æ¸©åº¦é¢„æµ‹
è§£å†³ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜ï¼ŒåŒ…å«è¯¦ç»†æ³¨é‡Š
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, regularizers
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import os
import warnings
from datetime import datetime
import matplotlib

warnings.filterwarnings('ignore')

# ==================== è§£å†³ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜ ====================
# æ–¹æ³•1: å°è¯•ä½¿ç”¨ç³»ç»Ÿè‡ªå¸¦çš„ä¸­æ–‡å­—ä½“
def setup_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œé¿å…è­¦å‘Š"""
    try:
        # åˆ—å‡ºå¸¸è§çš„ä¸­æ–‡å­—ä½“ï¼ˆæ ¹æ®ç³»ç»Ÿè°ƒæ•´ï¼‰
        chinese_fonts = [
            'DejaVu Sans',  # é€šå¸¸å¯ç”¨çš„å­—ä½“
            'Arial Unicode MS',
            'Microsoft YaHei',
            'SimHei',
            'STHeiti',
            'WenQuanYi Micro Hei',
            'Noto Sans CJK SC',
        ]
        
        # å°è¯•æ‰¾åˆ°å¯ç”¨çš„ä¸­æ–‡å­—ä½“
        available_fonts = [f.name for f in matplotlib.font_manager.fontManager.ttflist]
        
        for font in chinese_fonts:
            if any(font.lower() in f.lower() for f in available_fonts):
                # è®¾ç½®matplotlibä½¿ç”¨ä¸­æ–‡å­—ä½“
                plt.rcParams['font.sans-serif'] = [font]
                plt.rcParams['axes.unicode_minus'] = False  # æ­£ç¡®æ˜¾ç¤ºè´Ÿå·
                print(f"ä½¿ç”¨å­—ä½“: {font}")
                return True
        
        # å¦‚æœæ²¡æ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“ï¼Œå°†ä¸­æ–‡æ ‡ç­¾æ”¹ä¸ºè‹±æ–‡
        print("æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œå°†ä½¿ç”¨è‹±æ–‡æ ‡ç­¾")
        return False
        
    except Exception as e:
        print(f"å­—ä½“è®¾ç½®é”™è¯¯: {e}")
        return False

# è°ƒç”¨å­—ä½“è®¾ç½®å‡½æ•°
has_chinese_font = setup_chinese_font()

# ==================== GPUé…ç½® ====================
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print(f"GPUå¯ç”¨: {[gpu.name for gpu in gpus]}")
    except RuntimeError as e:
        print(f"GPUè®¾ç½®é”™è¯¯: {e}")

# è®¾ç½®éšæœºç§å­ç¡®ä¿å¯å¤ç°æ€§
tf.random.set_seed(42)
np.random.seed(42)

print("=" * 70)
print("æ”¹è¿›ç‰ˆLSTMæ¨¡å‹ - å¾·é‡Œæ°”å€™æ¸©åº¦é¢„æµ‹")
print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("=" * 70)

# ==================== 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç† ====================
def load_and_preprocess():
    """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
    print("\n[1/6] æ­£åœ¨åŠ è½½æ°”å€™æ•°æ®é›†...")
    
    train_path = '../data/DailyDelhiClimateTrain.csv'
    test_path = '../data/DailyDelhiClimateTest.csv'
    
    try:
        # åŠ è½½è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        train = pd.read_csv(train_path)
        test = pd.read_csv(test_path)
        
        print(f"è®­ç»ƒé›†: {train.shape}ï¼Œæµ‹è¯•é›†: {test.shape}")
        
        # åˆå¹¶æ•°æ®ç”¨äºç»Ÿä¸€é¢„å¤„ç†
        combined = pd.concat([train, test], ignore_index=True)
        combined['date'] = pd.to_datetime(combined['date'])
        combined = combined.sort_values('date').reset_index(drop=True)
        
        print(f"æ€»æ•°æ®é‡: {len(combined)} å¤©")
        print(f"è®­ç»ƒé›†å æ¯”: {len(train)/len(combined)*100:.1f}%")
        print(f"æµ‹è¯•é›†å æ¯”: {len(test)/len(combined)*100:.1f}%")
        
        return combined, len(train)
        
    except Exception as e:
        print(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None, 0

# ==================== 2. ç‰¹å¾å·¥ç¨‹å’Œåºåˆ—åˆ›å»º ====================
def create_enhanced_sequences(df, window_size=14):
    """åˆ›å»ºå¢å¼ºçš„æ—¶é—´åºåˆ—ç‰¹å¾"""
    
    print(f"\n[2/6] åˆ›å»ºæ—¶é—´åºåˆ— (çª—å£å¤§å°={window_size})...")
    
    # åŸºç¡€ç‰¹å¾ï¼šæ¹¿åº¦ã€æ°”å‹ã€é£é€Ÿ
    base_features = ['humidity', 'meanpressure', 'wind_speed']
    
    # åˆ›å»ºç‰¹å¾å‰¯æœ¬
    df_features = df.copy()
    
    # ===== ç‰¹å¾å·¥ç¨‹ =====
    # 1. æ—¶é—´ç‰¹å¾
    df_features['month'] = df_features['date'].dt.month
    df_features['day_of_year'] = df_features['date'].dt.dayofyear
    df_features['day_of_week'] = df_features['date'].dt.dayofweek
    
    # 2. æ»åç‰¹å¾ (å‰1å¤©ã€å‰7å¤©æ¸©åº¦)
    df_features['temp_lag1'] = df_features['meantemp'].shift(1)
    df_features['temp_lag7'] = df_features['meantemp'].shift(7)
    
    # 3. æ»‘åŠ¨çª—å£ç»Ÿè®¡
    df_features['temp_rolling_mean_7'] = df_features['meantemp'].rolling(window=7, min_periods=1).mean()
    df_features['humidity_rolling_mean_7'] = df_features['humidity'].rolling(window=7, min_periods=1).mean()
    
    # 4. å·®å€¼ç‰¹å¾
    df_features['temp_diff_1'] = df_features['meantemp'].diff(1)
    df_features['humidity_diff_1'] = df_features['humidity'].diff(1)
    
    # å¡«å……ç¼ºå¤±å€¼ï¼ˆç”±shiftå’Œrollingäº§ç”Ÿï¼‰
    df_features = df_features.fillna(method='bfill').fillna(method='ffill')
    
    # æœ€ç»ˆç‰¹å¾é›†ï¼ˆé€‰æ‹©æœ€é‡è¦çš„ç‰¹å¾ï¼‰
    features = base_features + [
        'month', 'day_of_year', 
        'temp_lag1', 'temp_lag7',
        'temp_rolling_mean_7'
    ]
    
    print(f"ä½¿ç”¨ {len(features)} ä¸ªç‰¹å¾")
    print(f"ç‰¹å¾åˆ—è¡¨: {features}")
    
    # åˆ›å»ºæ—¶é—´åºåˆ—
    X, y = [], []
    for i in range(window_size, len(df_features)):
        X.append(df_features[features].iloc[i-window_size:i].values)
        y.append(df_features['meantemp'].iloc[i])
    
    X = np.array(X)
    y = np.array(y)
    
    print(f"åºåˆ—å½¢çŠ¶: X={X.shape}, y={y.shape}")
    print(f"æ¯ä¸ªæ ·æœ¬: {window_size}å¤© Ã— {len(features)}ä¸ªç‰¹å¾")
    
    return X, y, features

# ==================== 3. æ„å»ºæ”¹è¿›çš„LSTMæ¨¡å‹ ====================
def build_improved_lstm(input_shape):
    """æ„å»ºæ”¹è¿›çš„LSTMæ¨¡å‹æ¶æ„"""
    
    model = keras.Sequential([
        keras.Input(shape=input_shape),
        
        # ç¬¬ä¸€å±‚LSTM
        layers.LSTM(128, return_sequences=True,
                   kernel_regularizer=regularizers.l2(0.001),
                   recurrent_regularizer=regularizers.l2(0.001)),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        
        # ç¬¬äºŒå±‚LSTM
        layers.LSTM(64, return_sequences=True,
                   kernel_regularizer=regularizers.l2(0.001)),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        
        # ç¬¬ä¸‰å±‚LSTM
        layers.LSTM(32, return_sequences=False,
                   kernel_regularizer=regularizers.l2(0.001)),
        layers.BatchNormalization(),
        layers.Dropout(0.2),
        
        # å…¨è¿æ¥å±‚
        layers.Dense(32, activation='relu',
                    kernel_regularizer=regularizers.l2(0.001)),
        layers.Dropout(0.2),
        layers.Dense(16, activation='relu'),
        layers.Dense(1)  # è¾“å‡ºå±‚ï¼Œå›å½’é—®é¢˜
    ])
    
    # ä¼˜åŒ–å™¨é…ç½®
    optimizer = keras.optimizers.Adam(
        learning_rate=0.001,
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-07
    )
    
    # ç¼–è¯‘æ¨¡å‹
    model.compile(
        optimizer=optimizer,
        loss='mse',           # å‡æ–¹è¯¯å·®æŸå¤±
        metrics=['mae', 'mse'] # ç›‘æ§æŒ‡æ ‡
    )
    
    return model

# ==================== 4. è®­ç»ƒå’Œè¯„ä¼° ====================
def train_and_evaluate_improved():
    """è®­ç»ƒå¹¶è¯„ä¼°æ”¹è¿›æ¨¡å‹"""
    
    # åŠ è½½æ•°æ®
    combined, train_size = load_and_preprocess()
    if combined is None:
        return None, None
    
    # åˆ›å»ºåºåˆ—
    window_size = 14
    X, y, features = create_enhanced_sequences(combined, window_size)
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train = X[:train_size - window_size]
    y_train = y[:train_size - window_size]
    X_test = X[train_size - window_size:]
    y_test = y[train_size - window_size:]
    
    print(f"\n[3/6] æ•°æ®é›†åˆ’åˆ†:")
    print(f"è®­ç»ƒé›†: {X_train.shape}")
    print(f"æµ‹è¯•é›†: {X_test.shape}")
    
    # æ•°æ®å½’ä¸€åŒ–ï¼ˆåªåœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆï¼ï¼‰
    print("\n[4/6] æ•°æ®å½’ä¸€åŒ–...")
    scaler_X = MinMaxScaler()
    scaler_y = MinMaxScaler()
    
    # é‡å¡‘æ•°æ®è¿›è¡Œå½’ä¸€åŒ–
    X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])
    X_test_reshaped = X_test.reshape(-1, X_test.shape[-1])
    
    X_train_scaled = scaler_X.fit_transform(X_train_reshaped).reshape(X_train.shape)
    X_test_scaled = scaler_X.transform(X_test_reshaped).reshape(X_test.shape)
    
    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
    y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()
    
    # æ„å»ºæ¨¡å‹
    print("\n[5/6] æ„å»ºæ”¹è¿›çš„LSTMæ¨¡å‹...")
    input_shape = (window_size, len(features))
    model = build_improved_lstm(input_shape)
    
    # æ‰“å°æ¨¡å‹æ‘˜è¦
    model.summary()
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs('../models', exist_ok=True)
    os.makedirs('../results', exist_ok=True)
    
    # å›è°ƒå‡½æ•°
    callbacks = [
        # æ—©åœæ³•
        keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=25,
            restore_best_weights=True,
            verbose=1
        ),
        # å­¦ä¹ ç‡è¡°å‡
        keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=10,
            min_lr=1e-6,
            verbose=1
        ),
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        keras.callbacks.ModelCheckpoint(
            '../models/lstm_improved_best.h5',
            monitor='val_loss',
            save_best_only=True,
            verbose=1
        )
    ]
    
    # è®­ç»ƒæ¨¡å‹
    print("\n[6/6] å¼€å§‹è®­ç»ƒæ¨¡å‹...")
    history = model.fit(
        X_train_scaled, y_train_scaled,
        batch_size=32,
        epochs=150,  # å¢åŠ epochæ•°
        validation_split=0.2,
        callbacks=callbacks,
        verbose=1
    )
    
    # ==================== 5. è¯„ä¼°æ¨¡å‹ ====================
    print("\n" + "="*60)
    print("æ¨¡å‹è¯„ä¼°")
    print("="*60)
    
    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    test_loss, test_mae, test_mse = model.evaluate(X_test_scaled, y_test_scaled, verbose=0)
    
    # è¿›è¡Œé¢„æµ‹
    y_pred_scaled = model.predict(X_test_scaled, verbose=0).flatten()
    
    # åå½’ä¸€åŒ–
    y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
    y_test_original = scaler_y.inverse_transform(y_test_scaled.reshape(-1, 1)).flatten()
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    mse = mean_squared_error(y_test_original, y_pred)
    mae = mean_absolute_error(y_test_original, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test_original, y_pred)
    
    print(f"\næµ‹è¯•é›†è¯„ä¼°ç»“æœ:")
    print(f"MSE (å‡æ–¹è¯¯å·®): {mse:.4f} (Â°C)Â²")
    print(f"RMSE (å‡æ–¹æ ¹è¯¯å·®): {rmse:.4f} Â°C")
    print(f"MAE (å¹³å‡ç»å¯¹è¯¯å·®): {mae:.4f} Â°C")
    print(f"RÂ² åˆ†æ•°: {r2:.4f}")
    
    # è¯„ä¼°æ ‡å‡†
    if mse < 2:
        print("  ç»“æœ: ä¼˜ç§€")
    elif mse < 5:
        print("  ç»“æœ: è‰¯å¥½")
    elif mse < 10:
        print("  ç»“æœ: ä¸€èˆ¬")
    else:
        print("  ç»“æœ: éœ€è¦æ”¹è¿›")
    
    # ==================== 6. å¯è§†åŒ–ç»“æœ ====================
    visualize_results(y_test_original, y_pred, history, mse, mae, rmse, r2)
    
    # ä¿å­˜æ¨¡å‹
    model.save('../models/lstm_improved_final.h5')
    
    print(f"\næ¨¡å‹å·²ä¿å­˜: ../models/lstm_improved_final.h5")
    
    return mse, model

# ==================== 7. å¯è§†åŒ–å‡½æ•° ====================
def visualize_results(y_true, y_pred, history, mse, mae, rmse, r2):
    """å¯è§†åŒ–è®­ç»ƒç»“æœå’Œé¢„æµ‹"""
    
    print("\nğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
    
    # æ ¹æ®å­—ä½“æ”¯æŒé€‰æ‹©æ ‡ç­¾è¯­è¨€
    if has_chinese_font:
        # ä¸­æ–‡æ ‡ç­¾
        labels = {
            'loss': 'æŸå¤±',
            'mae': 'å¹³å‡ç»å¯¹è¯¯å·®(MAE)',
            'epoch': 'è®­ç»ƒè½®æ•°(Epoch)',
            'train_loss': 'è®­ç»ƒæŸå¤±',
            'val_loss': 'éªŒè¯æŸå¤±',
            'train_mae': 'è®­ç»ƒMAE',
            'val_mae': 'éªŒè¯MAE',
            'true_temp': 'çœŸå®æ¸©åº¦',
            'pred_temp': 'é¢„æµ‹æ¸©åº¦',
            'sample_index': 'æ ·æœ¬ç´¢å¼•',
            'temperature': 'æ¸©åº¦ (Â°C)',
            'comparison': f'æ¸©åº¦é¢„æµ‹å¯¹æ¯”\nMSE={mse:.2f}, RÂ²={r2:.2f}',
            'scatter': 'çœŸå®å€¼ vs é¢„æµ‹å€¼',
            'true_temp_scatter': 'çœŸå®æ¸©åº¦ (Â°C)',
            'pred_temp_scatter': 'é¢„æµ‹æ¸©åº¦ (Â°C)',
            'error': 'é¢„æµ‹è¯¯å·®',
            'error_dist': 'é¢„æµ‹è¯¯å·®åˆ†å¸ƒ',
            'error_value': 'è¯¯å·® (Â°C)',
            'frequency': 'é¢‘æ•°',
            'error_mean': f'å¹³å‡è¯¯å·®: {mae:.2f}Â°C',
            'residual': 'æ®‹å·®å›¾',
            'pred_temp_residual': 'é¢„æµ‹æ¸©åº¦ (Â°C)',
            'residual_value': 'æ®‹å·® (çœŸå®-é¢„æµ‹) (Â°C)'
        }
    else:
        # è‹±æ–‡æ ‡ç­¾
        labels = {
            'loss': 'Loss',
            'mae': 'Mean Absolute Error (MAE)',
            'epoch': 'Epoch',
            'train_loss': 'Training Loss',
            'val_loss': 'Validation Loss',
            'train_mae': 'Training MAE',
            'val_mae': 'Validation MAE',
            'true_temp': 'True Temperature',
            'pred_temp': 'Predicted Temperature',
            'sample_index': 'Sample Index',
            'temperature': 'Temperature (Â°C)',
            'comparison': f'Temperature Prediction\nMSE={mse:.2f}, RÂ²={r2:.2f}',
            'scatter': 'True vs Predicted',
            'true_temp_scatter': 'True Temperature (Â°C)',
            'pred_temp_scatter': 'Predicted Temperature (Â°C)',
            'error': 'Prediction Error',
            'error_dist': 'Prediction Error Distribution',
            'error_value': 'Error (Â°C)',
            'frequency': 'Frequency',
            'error_mean': f'Mean Error: {mae:.2f}Â°C',
            'residual': 'Residual Plot',
            'pred_temp_residual': 'Predicted Temperature (Â°C)',
            'residual_value': 'Residual (True-Pred) (Â°C)'
        }
    
    # åˆ›å»º2x3çš„å­å›¾
    fig = plt.figure(figsize=(16, 10))
    
    # 1. è®­ç»ƒæŸå¤±æ›²çº¿
    ax1 = plt.subplot(2, 3, 1)
    ax1.plot(history.history['loss'], label=labels['train_loss'], linewidth=2, alpha=0.8)
    ax1.plot(history.history['val_loss'], label=labels['val_loss'], linewidth=2, alpha=0.8)
    ax1.set_xlabel(labels['epoch'])
    ax1.set_ylabel(labels['loss'])
    ax1.set_title('Training History - Loss')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. MAEæ›²çº¿
    ax2 = plt.subplot(2, 3, 2)
    ax2.plot(history.history['mae'], label=labels['train_mae'], linewidth=2, alpha=0.8)
    ax2.plot(history.history['val_mae'], label=labels['val_mae'], linewidth=2, alpha=0.8)
    ax2.set_xlabel(labels['epoch'])
    ax2.set_ylabel(labels['mae'])
    ax2.set_title('Training History - MAE')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. é¢„æµ‹å¯¹æ¯”ï¼ˆå‰100ä¸ªæ ·æœ¬ï¼‰
    ax3 = plt.subplot(2, 3, 3)
    n_show = min(100, len(y_true))
    ax3.plot(y_true[:n_show], label=labels['true_temp'], linewidth=2, alpha=0.8, color='blue')
    ax3.plot(y_pred[:n_show], label=labels['pred_temp'], linewidth=2, alpha=0.8, color='red')
    ax3.set_xlabel(labels['sample_index'])
    ax3.set_ylabel(labels['temperature'])
    ax3.set_title(labels['comparison'])
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. çœŸå®å€¼ vs é¢„æµ‹å€¼æ•£ç‚¹å›¾
    ax4 = plt.subplot(2, 3, 4)
    ax4.scatter(y_true, y_pred, alpha=0.5, s=20, color='green')
    # æ·»åŠ å®Œç¾é¢„æµ‹çº¿
    min_val = min(y_true.min(), y_pred.min())
    max_val = max(y_true.max(), y_pred.max())
    ax4.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')
    ax4.set_xlabel(labels['true_temp_scatter'])
    ax4.set_ylabel(labels['pred_temp_scatter'])
    ax4.set_title(labels['scatter'])
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    # 5. è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾
    ax5 = plt.subplot(2, 3, 5)
    errors = y_true - y_pred
    ax5.hist(errors, bins=30, edgecolor='black', alpha=0.7, color='orange')
    ax5.axvline(x=0, color='r', linestyle='--', linewidth=2)
    ax5.set_xlabel(labels['error_value'])
    ax5.set_ylabel(labels['frequency'])
    ax5.set_title(f"{labels['error_dist']}\n{labels['error_mean']}")
    ax5.grid(True, alpha=0.3)
    
    # 6. æ®‹å·®å›¾
    ax6 = plt.subplot(2, 3, 6)
    ax6.scatter(y_pred, errors, alpha=0.5, s=20, color='purple')
    ax6.axhline(y=0, color='r', linestyle='--', linewidth=2)
    ax6.set_xlabel(labels['pred_temp_residual'])
    ax6.set_ylabel(labels['residual_value'])
    ax6.set_title(labels['residual'])
    ax6.grid(True, alpha=0.3)
    
    plt.suptitle(f'LSTM Model Results - RMSE: {rmse:.2f}Â°C, MAE: {mae:.2f}Â°C', fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    # ä¿å­˜å›¾åƒ
    plt.savefig('../results/lstm_improved_results.png', dpi=150, bbox_inches='tight')
    print("å¯è§†åŒ–ç»“æœå·²ä¿å­˜: ../results/lstm_improved_results.png")
    plt.show()
    
    # ä¿å­˜é¢„æµ‹ç»“æœä¸ºCSV
    results_df = pd.DataFrame({
        'True_Temperature': y_true,
        'Predicted_Temperature': y_pred,
        'Error': errors,
        'Absolute_Error': np.abs(errors)
    })
    results_df.to_csv('../results/lstm_predictions.csv', index=False)
    print("é¢„æµ‹ç»“æœå·²ä¿å­˜: ../results/lstm_predictions.csv")

# ==================== ä¸»ç¨‹åº ====================
if __name__ == "__main__":
    try:
        print("å¼€å§‹è®­ç»ƒæ”¹è¿›ç‰ˆLSTMæ¨¡å‹...")
        mse, model = train_and_evaluate_improved()
        
        if mse is not None:
            print(f"\n" + "="*70)
            print("è®­ç»ƒå®Œæˆï¼")
            print("="*70)
            print(f"æœ€ç»ˆæµ‹è¯•é›†MSE: {mse:.4f} (Â°C)Â²")
            print(f"RMSE: {np.sqrt(mse):.4f} Â°C")
            print(f"å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print("="*70)
            
            # ç»“æœè¯„ä¼°
            if mse < 5:
                print("\nç»“æœä¼˜ç§€ï¼æ¨¡å‹æ€§èƒ½å¾ˆå¥½ã€‚")
            elif mse < 10:
                print("\nç»“æœè‰¯å¥½ï¼è¾¾åˆ°äº†ä½œä¸šè¦æ±‚ã€‚")
            else:
                print("\nç»“æœä¸€èˆ¬ï¼Œå¯ä»¥è€ƒè™‘è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚")
                
        else:
            print("\nè®­ç»ƒå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")
            
    except Exception as e:
        print(f"\nè¿è¡Œæ—¶é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        print("\nå¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        print("1. æ£€æŸ¥æ•°æ®é›†è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("2. ç¡®ä¿æœ‰è¶³å¤Ÿçš„GPUå†…å­˜")
        print("3. æ£€æŸ¥PythonåŒ…æ˜¯å¦å®‰è£…å®Œæ•´")