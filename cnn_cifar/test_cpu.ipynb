{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "852de94d",
   "metadata": {},
   "source": [
    "# 在 CPU 环境下运行的 CIFAR-10 ResNet 训练笔记本\n",
    "# 保持原有训练方式与精度，提供详细计时与日志输出\n",
    "\n",
    "# 目录（按顺序运行）\n",
    "# 1. 环境与依赖检查（CPU 运行）\n",
    "# 2. 数据预处理与加载器（CPU 设置）\n",
    "# 3. 模型定义：带残差块的 ResNet_CIFAR10\n",
    "# 4. 训练与测试函数：详细日志与计时\n",
    "# 5. 主程序：CPU 训练流程、学习率调度与模型保存\n",
    "# 6. 附录：执行顺序说明\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf98fa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 环境与依赖检查（CPU 运行）\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  # 绘制训练曲线\n",
    "\n",
    "# 强制使用CPU，在线平台通常无GPU\n",
    "device = torch.device('cpu')\n",
    "print(f\"使用设备: {device}\")\n",
    "print(f\"PyTorch 版本: {torch.__version__}\")\n",
    "\n",
    "# 设定随机种子，保证可复现\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = False  # CPU 无效，但保持显式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfecf085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 数据预处理与加载器（CPU 设置）\n",
    "# 使用与原脚本一致的增强：随机裁剪+水平翻转+标准化\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# 下载/加载 CIFAR-10 数据集到 ./data 目录\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# DataLoader：在线平台兼容性，num_workers=0，pin_memory=False（CPU）\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "print(f\"训练集大小: {len(train_dataset)}, 测试集大小: {len(test_dataset)}\")\n",
    "print(f\"批量大小: {batch_size}, 迭代步数/epoch: {len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583e41ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 模型定义：带残差块的 ResNet_CIFAR10\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"基本残差块：两层3x3卷积 + 可选1x1 shortcut，用于对齐通道/步幅\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # 若步幅或通道不一致，使用1x1卷积调整shortcut形状\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNetCIFAR10(nn.Module):\n",
    "    \"\"\"四个stage的简化 ResNet：通道 64/128/256/512，对应 stride 1/2/2/2\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(64, num_blocks=2, stride=1)\n",
    "        self.layer2 = self._make_layer(128, num_blocks=2, stride=2)\n",
    "        self.layer3 = self._make_layer(256, num_blocks=2, stride=2)\n",
    "        self.layer4 = self._make_layer(512, num_blocks=2, stride=2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(ResidualBlock(self.in_channels, out_channels, s))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 实例化模型到CPU\n",
    "device_model = ResNetCIFAR10(num_classes=10).to(device)\n",
    "print(device_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de10c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 训练与测试函数：详细日志与计时\n",
    "def train(model, train_loader, criterion, optimizer, epoch, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        # 适度打印中间进度，避免过多刷屏\n",
    "        if (batch_idx + 1) % 100 == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            acc = 100.0 * correct / total\n",
    "            print(f\"  [Epoch {epoch:03d}][Batch {batch_idx+1:04d}/{len(train_loader):04d}] \"\n",
    "                  f\"Loss {avg_loss:.4f} | Acc {acc:.2f}%\")\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    epoch_loss = total_loss / len(train_loader)\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    samples_per_sec = total / elapsed if elapsed > 0 else 0.0\n",
    "    return epoch_acc, epoch_loss, elapsed, samples_per_sec\n",
    "\n",
    "\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    test_loss = total_loss / len(test_loader)\n",
    "    test_acc = 100.0 * correct / total\n",
    "    return test_acc, test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a1d704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 主程序：CPU 训练流程、学习率调度与模型保存\n",
    "num_epochs = 200\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(device_model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "best_test_acc = 0.0\n",
    "best_model_path = 'resnet_cifar10_cpu_best.pth'\n",
    "\n",
    "# 记录训练历史用于可视化\n",
    "train_loss_hist, train_acc_hist = [], []\n",
    "test_loss_hist, test_acc_hist = [], []\n",
    "\n",
    "print(\"开始CPU训练（无GPU加速，时间可能较长）...\")\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    train_acc, train_loss, elapsed, throughput = train(device_model, train_loader, criterion, optimizer, epoch, device)\n",
    "    test_acc, test_loss = test(device_model, test_loader, criterion, device)\n",
    "    scheduler.step()\n",
    "\n",
    "    # 记录历史\n",
    "    train_loss_hist.append(train_loss)\n",
    "    train_acc_hist.append(train_acc)\n",
    "    test_loss_hist.append(test_loss)\n",
    "    test_acc_hist.append(test_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | LR {current_lr:.5f} | \"\n",
    "          f\"Train Loss {train_loss:.4f} Acc {train_acc:.2f}% | \"\n",
    "          f\"Test Loss {test_loss:.4f} Acc {test_acc:.2f}% | \"\n",
    "          f\"Epoch Time {elapsed:0.2f}s | Throughput {throughput:0.1f} samples/s\")\n",
    "\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': device_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_test_acc': best_test_acc\n",
    "        }, best_model_path)\n",
    "        print(f\"  >>> 保存最优模型至 {best_model_path}，当前最佳测试准确率: {best_test_acc:.2f}%\")\n",
    "\n",
    "print(f\"训练结束，最佳测试准确率: {best_test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb2b2b4",
   "metadata": {},
   "source": [
    "# 6. 附录：执行顺序说明\n",
    "# 按以下顺序逐个运行上方单元：\n",
    "# 1) 环境与依赖检查（CPU 运行）\n",
    "# 2) 数据预处理与加载器（CPU 设置）\n",
    "# 3) 模型定义：带残差块的 ResNet_CIFAR10\n",
    "# 4) 训练与测试函数：详细日志与计时\n",
    "# 5) 主程序：CPU 训练流程、学习率调度与模型保存\n",
    "# 运行完毕后，最佳模型保存在当前目录: resnet_cifar10_cpu_best.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340a3b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 可视化训练曲线（英文标题/标签，中文注释）\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss 曲线\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss_hist, label='Train Loss', color='steelblue', linewidth=2)\n",
    "plt.plot(test_loss_hist, label='Test Loss', color='darkorange', linewidth=2)\n",
    "plt.title('Loss Curves', fontsize=14)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy 曲线\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_acc_hist, label='Train Acc', color='seagreen', linewidth=2)\n",
    "plt.plot(test_acc_hist, label='Test Acc', color='firebrick', linewidth=2)\n",
    "plt.title('Accuracy Curves', fontsize=14)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "print(\"Saved training curves to training_curves.png\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
